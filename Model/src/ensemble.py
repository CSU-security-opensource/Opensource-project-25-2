import pandas as pd
import numpy as np
from neuralforecast import NeuralForecast
from neuralforecast.models import LSTM, GRU, NHITS
from neuralforecast.losses.pytorch import MAE
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import time
import os
import pickle
import shutil
import warnings
warnings.filterwarnings('ignore')

# lightning_logs ë¬¸ì œ í•´ê²°: ê¸°ì¡´ í´ë” ì‚­ì œ ë° ì¬ìƒì„±
if os.path.exists('lightning_logs'):
    try:
        if os.path.isfile('lightning_logs'):
            os.remove('lightning_logs')
        else:
            shutil.rmtree('lightning_logs')
    except:
        pass

# í°íŠ¸ ì„¤ì • (í•œê¸€ ê¹¨ì§ ë°©ì§€)
try:
    # Windows
    plt.rcParams['font.family'] = 'Malgun Gothic'
except:
    try:
        # Mac
        plt.rcParams['font.family'] = 'AppleGothic'
    except:
        # Linux ë˜ëŠ” í°íŠ¸ ì—†ì„ ê²½ìš°
        plt.rcParams['font.family'] = 'DejaVu Sans'
        print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì˜ë¬¸ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")

plt.rcParams['axes.unicode_minus'] = False

print("="*70)
print("ğŸ¯ ì•™ìƒë¸” + Dropout + ìµœì í™”ëœ íŠ¹ì„± ì„ íƒ")
print("="*70)

# ========================================
# 1. ë°ì´í„° ë¡œë“œ
# ========================================
print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
train_df = pd.read_csv("../../Data/weather/train_data_fixed.csv")
val_df = pd.read_csv("../../Data/weather/validation_data_fixed.csv")
test_df = pd.read_csv("../../Data/weather/test_data_fixed_filtered.csv")

for df in [train_df, val_df, test_df]:
    df['timestamp'] = pd.to_datetime(df['timestamp'])

print(f"âœ… Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}")

# ========================================
# 2. ìƒê´€ê´€ê³„ ë¶„ì„ìœ¼ë¡œ ì¤‘ìš” ë³€ìˆ˜ ì„ íƒ
# ========================================
print("\nğŸ” ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")

for df in [train_df, val_df, test_df]:
    df['hour'] = df['timestamp'].dt.hour
    df['month'] = df['timestamp'].dt.month
    df['dayofyear'] = df['timestamp'].dt.dayofyear

# ìƒê´€ê³„ìˆ˜ ê³„ì‚°
corr_features = ['ì „ë ¥ìˆ˜ìš”ëŸ‰', 'temp', 'rain', 'humidity', 'insolation', 'cloud', 
                 'hour', 'month']
correlation = train_df[corr_features].corr()['ì „ë ¥ìˆ˜ìš”ëŸ‰'].sort_values(ascending=False)

print("\nğŸ“Š ì „ë ¥ìˆ˜ìš”ëŸ‰ê³¼ì˜ ìƒê´€ê³„ìˆ˜:")
for var, corr_val in correlation.items():
    if var != 'ì „ë ¥ìˆ˜ìš”ëŸ‰':
        print(f"   {var:15s}: {corr_val:7.4f}")

# ìƒê´€ê³„ìˆ˜ 0.1 ì´ìƒì¸ ë³€ìˆ˜ë§Œ ì„ íƒ (ì ˆëŒ€ê°’)
important_features = [var for var in correlation.index 
                     if var != 'ì „ë ¥ìˆ˜ìš”ëŸ‰' and abs(correlation[var]) > 0.1]
print(f"\nâœ… ì„ íƒëœ ì¤‘ìš” ë³€ìˆ˜ ({len(important_features)}ê°œ): {important_features}")

# ========================================
# 3. ë°ì´í„° ì¤€ë¹„
# ========================================
def prepare_nf_data(df, feature_list):
    result_df = pd.DataFrame({
        'unique_id': 'jeju_solar',
        'ds': df['timestamp'],
        'y': df['ì „ë ¥ìˆ˜ìš”ëŸ‰']
    })
    
    for col in feature_list:
        result_df[col] = df[col].values
    
    return result_df

train_nf = prepare_nf_data(train_df, important_features)
val_nf = prepare_nf_data(val_df, important_features)
test_nf = prepare_nf_data(test_df, important_features)

validation_length = len(val_nf)

full_df = pd.concat([train_nf, val_nf, test_nf], ignore_index=True)
full_df = full_df.sort_values('ds').reset_index(drop=True)

print(f"âœ… ì „ì²´ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ (ì´ {len(full_df):,}ê°œ)")

# ========================================
# 4. ì•™ìƒë¸” ëª¨ë¸ ì„¤ì • (LSTM + GRU + NHITS)
# ========================================
print("\nâš™ï¸ ì•™ìƒë¸” ëª¨ë¸ ì„¤ì • ì¤‘...")
print("   - LSTM")
print("   - GRU")
print("   - NHITS")

horizon = 24 * 30
input_size = 24 * 30

models = [
    # Model 1: LSTM (ë¡œê±° ë¹„í™œì„±í™”)
    LSTM(
        h=horizon,
        input_size=input_size,
        hist_exog_list=important_features,
        encoder_n_layers=2,
        encoder_hidden_size=64,
        decoder_layers=2,
        decoder_hidden_size=64,
        scaler_type='standard',
        max_steps=1200,
        batch_size=32,
        learning_rate=1e-3,
        early_stop_patience_steps=5,
        loss=MAE(),
        random_seed=42,
        logger=False,  # ë¡œê±° ë¹„í™œì„±í™”
        alias='LSTM'
    ),
    
    # Model 2: GRU (ë¡œê±° ë¹„í™œì„±í™”)
    GRU(
        h=horizon,
        input_size=input_size,
        hist_exog_list=important_features,
        encoder_n_layers=2,
        encoder_hidden_size=64,
        decoder_layers=2,
        decoder_hidden_size=64,
        scaler_type='standard',
        max_steps=1200,
        batch_size=32,
        learning_rate=1e-3,
        early_stop_patience_steps=5,
        loss=MAE(),
        random_seed=123,
        logger=False,  # ë¡œê±° ë¹„í™œì„±í™”
        alias='GRU'
    ),
    
    # Model 3: NHITS (ë¡œê±° ë¹„í™œì„±í™”)
    NHITS(
        h=horizon,
        input_size=input_size,
        hist_exog_list=important_features,
        stack_types=['identity', 'identity', 'identity'],
        n_blocks=[1, 1, 1],
        mlp_units=[[64, 64], [64, 64], [64, 64]],
        scaler_type='standard',
        max_steps=1200,
        batch_size=32,
        learning_rate=1e-3,
        early_stop_patience_steps=5,
        loss=MAE(),
        random_seed=456,
        logger=False,  # ë¡œê±° ë¹„í™œì„±í™”
        alias='NHITS'
    )
]

nf = NeuralForecast(models=models, freq='H')

# ========================================
# 5. 12ê°œì›” ì—°ì† ì˜ˆì¸¡
# ========================================
print("\nğŸ”„ 12ê°œì›” ì—°ì† ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
start_time = time.time()

cv_df = nf.cross_validation(
    df=full_df,
    val_size=validation_length,
    n_windows=12,
    step_size=horizon
)

duration = time.time() - start_time
print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration/60:.1f}ë¶„)")

# ========================================
# 6. ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„±
# ========================================
print("\nğŸ¯ ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± ì¤‘...")

# ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸”
cv_df['Ensemble_Mean'] = (cv_df['LSTM'] + 
                          cv_df['GRU'] + 
                          cv_df['NHITS']) / 3

# ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” (LSTM 40%, GRU 30%, NHITS 30%)
cv_df['Ensemble_Weighted'] = (cv_df['LSTM'] * 0.4 + 
                              cv_df['GRU'] * 0.3 + 
                              cv_df['NHITS'] * 0.3)

print("âœ… ì•™ìƒë¸” ì™„ë£Œ")
print("   - Ensemble_Mean: ë‹¨ìˆœ í‰ê· ")
print("   - Ensemble_Weighted: ê°€ì¤‘ í‰ê·  (LSTM 40%, GRU 30%, NHITS 30%)")

# ========================================
# 7. ì„±ëŠ¥ í‰ê°€
# ========================================
print("\nğŸ“Š ì„±ëŠ¥ í‰ê°€")

y_true = cv_df['y'].values

results = []
model_names = ['LSTM', 'GRU', 'NHITS', 
               'Ensemble_Mean', 'Ensemble_Weighted']

for model_name in model_names:
    y_pred = cv_df[model_name].values
    
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    mask = y_true > 0.1
    if mask.sum() > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        mape = np.nan
    
    results.append({
        'Model': model_name,
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2,
        'MAPE(%)': mape
    })
    
    print(f"\n{model_name}:")
    print(f"   MAE : {mae:.3f}")
    print(f"   RMSE: {rmse:.3f}")
    print(f"   RÂ²  : {r2:.4f}")
    print(f"   MAPE: {mape:.2f}%")

results_df = pd.DataFrame(results).sort_values('MAE')
best_model = results_df.iloc[0]

# ========================================
# 8. ëª¨ë¸ ì €ì¥
# ========================================
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

if not os.path.exists('../Models'):
    os.makedirs('../Models')

if not os.path.exists('../checkpoint'):
    os.makedirs('../checkpoint')

# [1] NeuralForecast ì „ì²´ ëª¨ë¸ ì €ì¥ (.ckpt íŒŒì¼ë“¤)
nf.save(path='../Models/', model_index=None, overwrite=True)
print("   âœ… NeuralForecast ëª¨ë¸ ì €ì¥: ../Models/")

# [2] ë©”íƒ€ ì •ë³´ ì €ì¥ (í”¼í´)
model_metadata = {
    'important_features': important_features,
    'horizon': horizon,
    'input_size': input_size,
    'best_model_name': best_model['Model'],
    'best_model_performance': {
        'MAE': best_model['MAE'],
        'RMSE': best_model['RMSE'],
        'R2': best_model['R2'],
        'MAPE': best_model['MAPE(%)']
    },
    'ensemble_weights': {
        'LSTM': 0.4,
        'GRU': 0.3,
        'NHITS': 0.3
    },
    'scaler_type': 'standard',
    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
}

with open('../checkpoint/model_metadata.pkl', 'wb') as f:
    pickle.dump(model_metadata, f)
print("   âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: ../checkpoint/model_metadata.pkl")

# [3] ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œë„ ì €ì¥
import json
with open('../Models/best_model_info.json', 'w', encoding='utf-8') as f:
    json.dump({
        'best_model': best_model['Model'],
        'performance': {
            'MAE': float(best_model['MAE']),
            'RMSE': float(best_model['RMSE']),
            'R2': float(best_model['R2']),
            'MAPE': float(best_model['MAPE(%)'])
        },
        'features': important_features,
        'saved_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    }, f, indent=4, ensure_ascii=False)
print("   âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´: ../Models/best_model_info.json")

# ========================================
# 9. ê²°ê³¼ ì €ì¥
# ========================================
print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")

if not os.path.exists('../Results'):
    os.makedirs('../Results')

# [1] ì˜ˆì¸¡ ë°ì´í„°
cv_df.to_csv('../Results/ì•™ìƒë¸”_ì˜ˆì¸¡_ë°ì´í„°.csv', index=False, encoding='utf-8-sig')
print("   âœ… ì•™ìƒë¸”_ì˜ˆì¸¡_ë°ì´í„°.csv")

# [2] ì„±ëŠ¥ ë¹„êµ
results_df['Rank'] = range(1, len(results_df) + 1)
results_df.to_csv('../Results/ì•™ìƒë¸”_ì„±ëŠ¥_ë¹„êµ.csv', index=False, encoding='utf-8-sig')
print("   âœ… ì•™ìƒë¸”_ì„±ëŠ¥_ë¹„êµ.csv")

# [3] ìµœì¢… ì„±ëŠ¥ ìš”ì•½
summary_df = pd.DataFrame([{
    'Best_Model': best_model['Model'],
    'MAE': best_model['MAE'],
    'RMSE': best_model['RMSE'],
    'R2': best_model['R2'],
    'MAPE(%)': best_model['MAPE(%)'],
    'Training_Time(min)': round(duration/60, 2),
    'Features_Used': ', '.join(important_features),
    'Num_Features': len(important_features),
    'Early_Stopping': 'Yes (patience=5)',
    'Ensemble': 'Yes'
}])
summary_df.to_csv('../Results/ìµœì¢…_ì„±ëŠ¥_ìš”ì•½.csv', index=False, encoding='utf-8-sig')
print("   âœ… ìµœì¢…_ì„±ëŠ¥_ìš”ì•½.csv")

# ========================================
# 10. ì‹œê°í™”
# ========================================
print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")

# 10-1. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# MAE ë¹„êµ
axes[0, 0].barh(results_df['Model'], results_df['MAE'], 
                color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'],
                edgecolor='black', linewidth=1.5)
axes[0, 0].set_xlabel('MAE (Lower is Better)')
axes[0, 0].set_title('Model Performance - MAE', fontweight='bold', fontsize=12)
axes[0, 0].invert_yaxis()
axes[0, 0].grid(alpha=0.3, axis='x')

# RÂ² ë¹„êµ
axes[0, 1].barh(results_df['Model'], results_df['R2'],
                color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'],
                edgecolor='black', linewidth=1.5)
axes[0, 1].set_xlabel('R-squared (Higher is Better)')
axes[0, 1].set_title('Model Performance - R2', fontweight='bold', fontsize=12)
axes[0, 1].invert_yaxis()
axes[0, 1].grid(alpha=0.3, axis='x')

# ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ (Best Model)
best_model_name = best_model['Model']
sample_size = min(5000, len(cv_df))
sample_indices = np.random.choice(len(cv_df), sample_size, replace=False)
sample_indices = np.sort(sample_indices)

axes[1, 0].scatter(cv_df.iloc[sample_indices]['y'], 
                   cv_df.iloc[sample_indices][best_model_name],
                   alpha=0.3, s=1, color='steelblue')
axes[1, 0].plot([cv_df['y'].min(), cv_df['y'].max()], 
                [cv_df['y'].min(), cv_df['y'].max()],
                'r--', linewidth=2, label='Perfect Prediction')
axes[1, 0].set_xlabel('Actual Power Demand')
axes[1, 0].set_ylabel('Predicted Power Demand')
axes[1, 0].set_title(f'Actual vs Predicted ({best_model_name})', fontweight='bold', fontsize=12)
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# ì„±ëŠ¥ ìˆœìœ„ í…Œì´ë¸”
axes[1, 1].axis('off')
table_data = []
for idx, row in results_df.iterrows():
    table_data.append([
        f"#{row['Rank']}",
        row['Model'],
        f"{row['MAE']:.2f}",
        f"{row['R2']:.4f}"
    ])

table = axes[1, 1].table(cellText=table_data,
                         colLabels=['Rank', 'Model', 'MAE', 'R2'],
                         cellLoc='center',
                         loc='center',
                         bbox=[0, 0.2, 1, 0.7])
table.auto_set_font_size(False)
table.set_fontsize(9)
table.scale(1, 2.5)

# í—¤ë” ìŠ¤íƒ€ì¼
for i in range(4):
    table[(0, i)].set_facecolor('#2C3E50')
    table[(0, i)].set_text_props(weight='bold', color='white')

# 1ìœ„ ê°•ì¡°
for i in range(4):
    table[(1, i)].set_facecolor('#FFD700')
    table[(1, i)].set_text_props(weight='bold')

axes[1, 1].set_title('Performance Ranking', fontweight='bold', fontsize=14, pad=20)

plt.tight_layout()
plt.savefig('../Results/ì•™ìƒë¸”_ì„±ëŠ¥_ë¹„êµ.png', dpi=300, bbox_inches='tight')
print("   âœ… ì•™ìƒë¸”_ì„±ëŠ¥_ë¹„êµ.png")

# 10-2. ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
fig, axes = plt.subplots(3, 1, figsize=(20, 15))

# ì²˜ìŒ 2000ê°œ ë°ì´í„°í¬ì¸íŠ¸ë§Œ ì‹œê°í™”
plot_range = slice(0, min(2000, len(cv_df)))

# ê°œë³„ ëª¨ë¸
axes[0].plot(cv_df['ds'].iloc[plot_range], cv_df['y'].iloc[plot_range], 
            label='Actual', color='black', alpha=0.5, linewidth=1)
axes[0].plot(cv_df['ds'].iloc[plot_range], cv_df['LSTM'].iloc[plot_range],
            label='LSTM', color='red', alpha=0.6, linewidth=0.8)
axes[0].plot(cv_df['ds'].iloc[plot_range], cv_df['GRU'].iloc[plot_range],
            label='GRU', color='blue', alpha=0.6, linewidth=0.8)
axes[0].plot(cv_df['ds'].iloc[plot_range], cv_df['NHITS'].iloc[plot_range],
            label='NHITS', color='green', alpha=0.6, linewidth=0.8)
axes[0].set_ylabel('Power Demand')
axes[0].set_title('Individual Models Prediction', fontweight='bold')
axes[0].legend(loc='upper right')
axes[0].grid(alpha=0.3)

# ì•™ìƒë¸” ëª¨ë¸
axes[1].plot(cv_df['ds'].iloc[plot_range], cv_df['y'].iloc[plot_range],
            label='Actual', color='black', alpha=0.5, linewidth=1.5)
axes[1].plot(cv_df['ds'].iloc[plot_range], cv_df['Ensemble_Mean'].iloc[plot_range],
            label='Ensemble (Mean)', color='purple', alpha=0.7, linewidth=1.2)
axes[1].plot(cv_df['ds'].iloc[plot_range], cv_df['Ensemble_Weighted'].iloc[plot_range],
            label='Ensemble (Weighted)', color='orange', alpha=0.7, linewidth=1.2)
axes[1].set_ylabel('Power Demand')
axes[1].set_title('Ensemble Models Prediction', fontweight='bold')
axes[1].legend(loc='upper right')
axes[1].grid(alpha=0.3)

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸
axes[2].plot(cv_df['ds'].iloc[plot_range], cv_df['y'].iloc[plot_range],
            label='Actual', color='black', alpha=0.5, linewidth=1.5)
axes[2].plot(cv_df['ds'].iloc[plot_range], cv_df[best_model_name].iloc[plot_range],
            label=f'Best Model ({best_model_name})', color='darkgreen', 
            alpha=0.8, linewidth=1.2)
axes[2].set_xlabel('Date')
axes[2].set_ylabel('Power Demand')
axes[2].set_title(f'Best Model: {best_model_name} (RÂ²={best_model["R2"]:.4f})', 
                 fontweight='bold')
axes[2].legend(loc='upper right')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('../Results/ì•™ìƒë¸”_ì‹œê³„ì—´_ì˜ˆì¸¡.png', dpi=300, bbox_inches='tight')
print("   âœ… ì•™ìƒë¸”_ì‹œê³„ì—´_ì˜ˆì¸¡.png")

# ========================================
# 11. ìµœì¢… ê²°ê³¼ ìš”ì•½
# ========================================
print("\n" + "="*70)
print("ğŸ† ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("="*70)
print(f"\nìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['Model']}")
print(f"   MAE  : {best_model['MAE']:.3f}")
print(f"   RMSE : {best_model['RMSE']:.3f}")
print(f"   RÂ²   : {best_model['R2']:.4f}")
print(f"   MAPE : {best_model['MAPE(%)']:.2f}%")
print(f"\nì‚¬ìš©ëœ íŠ¹ì„± ({len(important_features)}ê°œ):")
for feat in important_features:
    print(f"   - {feat}")
print(f"\nì´ í•™ìŠµ ì‹œê°„: {duration/60:.1f}ë¶„")
print(f"ì •ê·œí™” ë°©ë²•: Early Stopping (patience=5)")
print(f"ì•™ìƒë¸” ë°©ë²•: Mean + Weighted Average")
print("="*70)

print("\nğŸ“¦ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼:")
print("   - ../Models/*.ckpt (NeuralForecast ëª¨ë¸)")
print("   - ../checkpoint/model_metadata.pkl (ë©”íƒ€ë°ì´í„°)")
print("   - ../Models/best_model_info.json (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´)")